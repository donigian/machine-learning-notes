Feature Scaling
Make sure features are on similar scale. Elipsoids are too narrow and thus take longer for each step of gradient descent to converge. By scaling features, elipsoid becomes more uniformly shaped.
Get every feature into approx -1 <= X1 <= 1 range 

Mean Normalization
Replace Xi with Xi - Ui to make features have approx zero mean
(not apply to X0=1)

X1 = ( size - 1000 ) / 2000

or (X1 - U1) / S1 where S1 = max - min

Learning Rate Alpha
How to choose alpha to make sure gradient descent is working correctly.
Job of gradient descent is to minimize cost func J(theta). Value of cost func should be decreasing after every iteration if it's working properly.
J(theta) eventually flattens out. # of iterations could be different for each case.

You can define automatic converge tests if J(theta) decreases by 10^-3 after each iteration.
Use smaller alpha if gradient descent is not decreasing. If J(theta) is too small, gradient descent can be slow to converge.

Features & Polynomial Regression
Define new features to get better models.

Normal Eqn will solve for theta analytically, instead of iterative nature of gradient descent 

 gradient descent 
 + need to choose alpha
 + needs many iterations
 + works well with large n 

 Normal Eqn
 + no need to choose alpha
 + don't need to iterate
 + Need to compute (X^TX)^-1 cost of inverting an inverse is O(n^3), slow for large n.  Size of n determines which method to use.

